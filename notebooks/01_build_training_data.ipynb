{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016ca6aa",
   "metadata": {},
   "source": [
    "# Acoustic Shield - Training Data Pipeline\n",
    "\n",
    "This notebook orchestrates the complete data pipeline:\n",
    "1. Extract crash hotspots from GeoJSON\n",
    "2. Enrich with weather data (Open-Meteo API)\n",
    "3. Synthesize risk events\n",
    "4. Build audio generation recipes\n",
    "5. Run SageMaker Processing job to generate WAV files\n",
    "6. Validate outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1867be2",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import boto3\n",
    "from sagemaker import Session\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "from data_pipeline import (\n",
    "    S3Client,\n",
    "    HotspotExtractor,\n",
    "    WeatherEnricher,\n",
    "    RiskEventSynthesizer,\n",
    "    RecipeBuilder\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - No hard-coded regions!\n",
    "RAW_BUCKET = 'acousticshield-raw'\n",
    "ML_BUCKET = 'acousticshield-ml'\n",
    "CRASH_FILE_KEY = 'crash_hotspots/sanjose_crashes.geojson'\n",
    "SAGEMAKER_ROLE = 'role-sagemaker-processing'\n",
    "\n",
    "# Processing parameters - UPDATED TO GENERATE 1000+ FILES\n",
    "TOP_N_HOTSPOTS = 50  # Increased from 25 to 50\n",
    "EVENTS_PER_HOTSPOT = 5  # Increased from 4 to 5\n",
    "RECIPES_PER_EVENT = 4  # NEW: Generate 4 recipe variations per event\n",
    "\n",
    "# Expected output: 50 hotspots √ó 5 events √ó 4 recipes = 1000 audio files\n",
    "\n",
    "# Get region from bucket\n",
    "s3_client = S3Client()\n",
    "REGION = s3_client.get_bucket_region(RAW_BUCKET)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Raw bucket: {RAW_BUCKET}\")\n",
    "print(f\"  ML bucket: {ML_BUCKET}\")\n",
    "print(f\"  Region: {REGION}\")\n",
    "print(f\"  Crash file: s3://{RAW_BUCKET}/{CRASH_FILE_KEY}\")\n",
    "print(f\"  Top hotspots: {TOP_N_HOTSPOTS}\")\n",
    "print(f\"  Events per hotspot: {EVENTS_PER_HOTSPOT}\")\n",
    "print(f\"  Recipes per event: {RECIPES_PER_EVENT}\")\n",
    "print(f\"  Expected total files: {TOP_N_HOTSPOTS * EVENTS_PER_HOTSPOT * RECIPES_PER_EVENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdde042",
   "metadata": {},
   "source": [
    "## Step 1: Extract Crash Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crash data from S3\n",
    "logger.info(f\"Loading crash data from s3://{RAW_BUCKET}/{CRASH_FILE_KEY}\")\n",
    "crash_data = s3_client.read_json(RAW_BUCKET, CRASH_FILE_KEY)\n",
    "\n",
    "# Extract hotspots\n",
    "extractor = HotspotExtractor(crash_data)\n",
    "hotspots = extractor.extract_top_hotspots(top_n=TOP_N_HOTSPOTS)\n",
    "\n",
    "# Get summary stats\n",
    "stats = extractor.get_summary_stats()\n",
    "print(f\"\\nüìä Crash Data Summary:\")\n",
    "print(f\"  Total crashes: {stats['total_crashes']}\")\n",
    "print(f\"  Total injuries: {stats['total_injuries']}\")\n",
    "print(f\"  Total fatalities: {stats['total_fatalities']}\")\n",
    "print(f\"  Speeding involved: {stats['speeding_involved_pct']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Top 5 Hotspots:\")\n",
    "for hotspot in hotspots[:5]:\n",
    "    print(f\"  {hotspot['rank']}. {hotspot['location_name']}: {hotspot['crash_count']} crashes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855c995",
   "metadata": {},
   "source": [
    "## Step 2: Enrich with Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich hotspots with weather data from Open-Meteo API\n",
    "logger.info(\"Fetching weather data for hotspots...\")\n",
    "enricher = WeatherEnricher()\n",
    "enriched_hotspots = enricher.enrich_hotspots(hotspots, rate_limit_delay=0.5)\n",
    "\n",
    "# Show sample enriched data\n",
    "print(f\"\\nüå§Ô∏è  Sample Enriched Hotspot:\")\n",
    "sample = enriched_hotspots[0]\n",
    "print(f\"  Location: {sample['location_name']}\")\n",
    "print(f\"  Crashes: {sample['crash_count']}\")\n",
    "print(f\"  Weather:\")\n",
    "weather = sample['weather']\n",
    "print(f\"    Temperature: {weather['temperature_c']:.1f}¬∞C\")\n",
    "print(f\"    Rain: {weather['rain_mm']:.1f}mm\")\n",
    "print(f\"    Wind: {weather['wind_speed_kmh']:.1f} km/h\")\n",
    "print(f\"    Risk: {enricher.categorize_weather_risk(weather)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d68f76",
   "metadata": {},
   "source": [
    "## Step 3: Synthesize Risk Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic risk events\n",
    "logger.info(\"Synthesizing risk events...\")\n",
    "synthesizer = RiskEventSynthesizer(seed=42)\n",
    "risk_events = synthesizer.synthesize_events(enriched_hotspots, events_per_hotspot=EVENTS_PER_HOTSPOT)\n",
    "\n",
    "# Get distribution\n",
    "distribution = synthesizer.get_event_distribution(risk_events)\n",
    "print(f\"\\n‚ö†Ô∏è  Risk Event Distribution:\")\n",
    "print(f\"  Total events: {distribution['total_events']}\")\n",
    "print(f\"  By risk type:\")\n",
    "for risk_type, count in distribution['risk_type_distribution'].items():\n",
    "    print(f\"    {risk_type}: {count}\")\n",
    "print(f\"  By weather risk:\")\n",
    "for weather_risk, count in distribution['weather_risk_distribution'].items():\n",
    "    print(f\"    {weather_risk}: {count}\")\n",
    "\n",
    "# Show sample event\n",
    "print(f\"\\nüìã Sample Risk Event:\")\n",
    "sample_event = risk_events[0]\n",
    "print(json.dumps(sample_event, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63018e86",
   "metadata": {},
   "source": [
    "## Step 4: Build Audio Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d423e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build audio generation recipes with multiple variations per event\n",
    "logger.info(\"Building audio recipes...\")\n",
    "builder = RecipeBuilder()\n",
    "recipes = builder.build_recipes(risk_events, recipes_per_event=RECIPES_PER_EVENT)\n",
    "\n",
    "# Get summary\n",
    "summary = builder.get_recipe_summary(recipes)\n",
    "print(f\"\\nüéµ Audio Recipe Summary:\")\n",
    "print(f\"  Total recipes: {summary['total_recipes']}\")\n",
    "print(f\"  Total audio duration: {summary['total_audio_duration_minutes']:.2f} minutes\")\n",
    "print(f\"  By risk type:\")\n",
    "for risk_type, count in summary['risk_type_distribution'].items():\n",
    "    print(f\"    {risk_type}: {count} recipes\")\n",
    "\n",
    "# Show sample recipe\n",
    "print(f\"\\nüéº Sample Recipe:\")\n",
    "sample_recipe = recipes[0]\n",
    "print(json.dumps(sample_recipe, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ab018",
   "metadata": {},
   "source": [
    "## Step 5: Save Intermediate Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52765b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save risk events to S3\n",
    "logger.info(\"Saving risk events to S3...\")\n",
    "risk_events_key = 'risk_events/risk_events.json'\n",
    "s3_path = s3_client.write_json(risk_events, RAW_BUCKET, risk_events_key)\n",
    "print(f\"‚úì Risk events saved to: {s3_path}\")\n",
    "\n",
    "# Save recipes to S3\n",
    "logger.info(\"Saving recipes to S3...\")\n",
    "recipes_key = 'prompts/audio_recipes.json'\n",
    "s3_path = s3_client.write_json(recipes, RAW_BUCKET, recipes_key)\n",
    "print(f\"‚úì Recipes saved to: {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a681ea6",
   "metadata": {},
   "source": [
    "## Step 6: Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session (region-agnostic)\n",
    "boto_session = boto3.Session(region_name=REGION)\n",
    "sagemaker_session = Session(boto_session=boto_session)\n",
    "\n",
    "# Get IAM role ARN\n",
    "iam_client = boto_session.client('iam')\n",
    "role_response = iam_client.get_role(RoleName=SAGEMAKER_ROLE)\n",
    "role_arn = role_response['Role']['Arn']\n",
    "\n",
    "print(f\"SageMaker Session:\")\n",
    "print(f\"  Region: {REGION}\")\n",
    "print(f\"  Role: {role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf839629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure processing job\n",
    "processor = ScriptProcessor(\n",
    "    role=role_arn,\n",
    "    image_uri=f'763104351884.dkr.ecr.{REGION}.amazonaws.com/pytorch-training:2.0.1-cpu-py310',\n",
    "    command=['python3'],\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    base_job_name='acousticshield-audio-generation',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "print(\"‚úì Processor configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processing job\n",
    "logger.info(\"Starting SageMaker processing job...\")\n",
    "\n",
    "processor.run(\n",
    "    code='../processing/augment.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f's3://{RAW_BUCKET}/{recipes_key}',\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination=f's3://{ML_BUCKET}/train/',\n",
    "            output_name='training_audio'\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        '--recipe-dir', '/opt/ml/processing/input',\n",
    "        '--output-dir', '/opt/ml/processing/output'\n",
    "    ],\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Processing job completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1de0b",
   "metadata": {},
   "source": [
    "## Step 7: Validate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c64575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated WAV files\n",
    "logger.info(\"Listing generated audio files...\")\n",
    "output_files = s3_client.list_objects(ML_BUCKET, prefix='train/', max_keys=100)\n",
    "\n",
    "wav_files = [f for f in output_files if f.endswith('.wav')]\n",
    "\n",
    "print(f\"\\nüéß Generated Audio Files:\")\n",
    "print(f\"  Total WAV files: {len(wav_files)}\")\n",
    "print(f\"  \\nFirst 10 files:\")\n",
    "for i, file_key in enumerate(wav_files[:10], start=1):\n",
    "    print(f\"    {i}. s3://{ML_BUCKET}/{file_key}\")\n",
    "\n",
    "if len(wav_files) > 10:\n",
    "    print(f\"    ... and {len(wav_files) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d50d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count files by risk type\n",
    "risk_type_counts = {'normal': 0, 'tireskid': 0, 'emergencybraking': 0, 'collisionimminent': 0}\n",
    "\n",
    "for wav_file in wav_files:\n",
    "    filename = wav_file.lower()\n",
    "    for risk_type in risk_type_counts.keys():\n",
    "        if risk_type in filename:\n",
    "            risk_type_counts[risk_type] += 1\n",
    "            break\n",
    "\n",
    "print(f\"\\nüìä Audio Files by Risk Type:\")\n",
    "for risk_type, count in risk_type_counts.items():\n",
    "    print(f\"  {risk_type.title()}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9759dd9",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ACOUSTIC SHIELD DATA PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìç Crash Hotspots Analyzed: {len(hotspots)}\")\n",
    "print(f\"‚ö†Ô∏è  Risk Events Generated: {len(risk_events)}\")\n",
    "print(f\"üéµ Audio Recipes Created: {len(recipes)}\")\n",
    "print(f\"üéß WAV Files Generated: {len(wav_files)}\")\n",
    "print(f\"\\nüíæ Data Locations:\")\n",
    "print(f\"  Risk Events: s3://{RAW_BUCKET}/{risk_events_key}\")\n",
    "print(f\"  Recipes: s3://{RAW_BUCKET}/{recipes_key}\")\n",
    "print(f\"  Training Audio: s3://{ML_BUCKET}/train/\")\n",
    "print(f\"\\n‚úÖ Ready for model training!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
